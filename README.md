<div align="center">
  <h1>Enhancing Talking Head Clarity and Smoothness via Optical Flow, Super-Resolution, and Frame Interpolation</h1>
</div>

<div align="center">
  <a href='https://xinming-shu.github.io/Talking-Head-FGE/' target="_blank"><img src='https://img.shields.io/badge/Project-TalkingHead_FGE-green'></a>
</div>
<br>


<div align="center">
  <h2>Demos</h2>
</div>

### GFPGAN: Before and After Comparison
<video controls loop src="videos/gfpgan_cmp.mp4" muted="false"></video>
Download: [GFPGAN: Before and After Comparison](https://github.com/Xinming-Shu/Talking-Head-FGE/gfpgan_cmp.mp4)

### EMA-VFI: Comparison of Different Interpolation Methods
<video controls loop src="videos/wav2lip_compare_vfi.mp4" muted="false"></video>
Download: [EMA-VFI: Comparison of Different Interpolation Methods](https://github.com/Xinming-Shu/Talking-Head-FGE/wav2lip_compare_vfi.mp4)

### Before and After Introducing Optical Flow Constraints
<video controls loop src="videos/Obama_ft_sr.mp4" muted="false"></video>
Download: [Before and After Introducing Optical Flow Constraints](https://github.com/Xinming-Shu/Talking-Head-FGE/Obama_ft_sr.mp4)
<br>


<div align="center">
  <h2>Acknowledgements</h2>
</div>

- We use pre-trained Wav2Lip model from [this repository](https://github.com/Rudrabha/Wav2Lip/tree/master/evaluation).
- We use pre-trained FlowNet from [this repository](https://github.com/NVIDIA/flownet2-pytorch).
- We use GFPGAN model from [this repository](https://github.com/TencentARC/GFPGAN).
- We use EMA-VFI from [this repository](https://github.com/MCG-NJU/EMA-VFI).

Thanks to the authors of above repositories.
